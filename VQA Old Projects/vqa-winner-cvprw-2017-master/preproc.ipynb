{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pdb\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "maxInt = sys.maxsize\n",
    "while True:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "    \n",
    "\n",
    "import base64\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.data.path.append('data')\n",
    "nltk.download('punkt', download_dir='data')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "ta_path = os.path.join('data', 'v2_mscoco_train2014_annotations.json')\n",
    "va_path = os.path.join('data', 'v2_mscoco_val2014_annotations.json')\n",
    "tq_path = os.path.join('data', 'v2_OpenEnded_mscoco_train2014_questions.json')\n",
    "vq_path = os.path.join('data', 'v2_OpenEnded_mscoco_val2014_questions.json')\n",
    "glove_path = os.path.join('data', 'glove', 'glove.6B.300d.txt')\n",
    "vfeats_path = os.path.join('data', 'trainval_resnet101_faster_rcnn_genome_36.tsv')\n",
    "\n",
    "contractions = {\n",
    "    \"aint\": \"ain't\", \"arent\": \"aren't\", \"cant\": \"can't\", \"couldve\":\n",
    "    \"could've\", \"couldnt\": \"couldn't\", \"couldn'tve\": \"couldn't've\",\n",
    "    \"couldnt've\": \"couldn't've\", \"didnt\": \"didn't\", \"doesnt\":\n",
    "    \"doesn't\", \"dont\": \"don't\", \"hadnt\": \"hadn't\", \"hadnt've\":\n",
    "    \"hadn't've\", \"hadn'tve\": \"hadn't've\", \"hasnt\": \"hasn't\", \"havent\":\n",
    "    \"haven't\", \"hed\": \"he'd\", \"hed've\": \"he'd've\", \"he'dve\":\n",
    "    \"he'd've\", \"hes\": \"he's\", \"howd\": \"how'd\", \"howll\": \"how'll\",\n",
    "    \"hows\": \"how's\", \"Id've\": \"I'd've\", \"I'dve\": \"I'd've\", \"Im\":\n",
    "    \"I'm\", \"Ive\": \"I've\", \"isnt\": \"isn't\", \"itd\": \"it'd\", \"itd've\":\n",
    "    \"it'd've\", \"it'dve\": \"it'd've\", \"itll\": \"it'll\", \"let's\": \"let's\",\n",
    "    \"maam\": \"ma'am\", \"mightnt\": \"mightn't\", \"mightnt've\":\n",
    "    \"mightn't've\", \"mightn'tve\": \"mightn't've\", \"mightve\": \"might've\",\n",
    "    \"mustnt\": \"mustn't\", \"mustve\": \"must've\", \"neednt\": \"needn't\",\n",
    "    \"notve\": \"not've\", \"oclock\": \"o'clock\", \"oughtnt\": \"oughtn't\",\n",
    "    \"ow's'at\": \"'ow's'at\", \"'ows'at\": \"'ow's'at\", \"'ow'sat\":\n",
    "    \"'ow's'at\", \"shant\": \"shan't\", \"shed've\": \"she'd've\", \"she'dve\":\n",
    "    \"she'd've\", \"she's\": \"she's\", \"shouldve\": \"should've\", \"shouldnt\":\n",
    "    \"shouldn't\", \"shouldnt've\": \"shouldn't've\", \"shouldn'tve\":\n",
    "    \"shouldn't've\", \"somebody'd\": \"somebodyd\", \"somebodyd've\":\n",
    "    \"somebody'd've\", \"somebody'dve\": \"somebody'd've\", \"somebodyll\":\n",
    "    \"somebody'll\", \"somebodys\": \"somebody's\", \"someoned\": \"someone'd\",\n",
    "    \"someoned've\": \"someone'd've\", \"someone'dve\": \"someone'd've\",\n",
    "    \"someonell\": \"someone'll\", \"someones\": \"someone's\", \"somethingd\":\n",
    "    \"something'd\", \"somethingd've\": \"something'd've\", \"something'dve\":\n",
    "    \"something'd've\", \"somethingll\": \"something'll\", \"thats\":\n",
    "    \"that's\", \"thered\": \"there'd\", \"thered've\": \"there'd've\",\n",
    "    \"there'dve\": \"there'd've\", \"therere\": \"there're\", \"theres\":\n",
    "    \"there's\", \"theyd\": \"they'd\", \"theyd've\": \"they'd've\", \"they'dve\":\n",
    "    \"they'd've\", \"theyll\": \"they'll\", \"theyre\": \"they're\", \"theyve\":\n",
    "    \"they've\", \"twas\": \"'twas\", \"wasnt\": \"wasn't\", \"wed've\":\n",
    "    \"we'd've\", \"we'dve\": \"we'd've\", \"weve\": \"we've\", \"werent\":\n",
    "    \"weren't\", \"whatll\": \"what'll\", \"whatre\": \"what're\", \"whats\":\n",
    "    \"what's\", \"whatve\": \"what've\", \"whens\": \"when's\", \"whered\":\n",
    "    \"where'd\", \"wheres\": \"where's\", \"whereve\": \"where've\", \"whod\":\n",
    "    \"who'd\", \"whod've\": \"who'd've\", \"who'dve\": \"who'd've\", \"wholl\":\n",
    "    \"who'll\", \"whos\": \"who's\", \"whove\": \"who've\", \"whyll\": \"why'll\",\n",
    "    \"whyre\": \"why're\", \"whys\": \"why's\", \"wont\": \"won't\", \"wouldve\":\n",
    "    \"would've\", \"wouldnt\": \"wouldn't\", \"wouldnt've\": \"wouldn't've\",\n",
    "    \"wouldn'tve\": \"wouldn't've\", \"yall\": \"y'all\", \"yall'll\":\n",
    "    \"y'all'll\", \"y'allll\": \"y'all'll\", \"yall'd've\": \"y'all'd've\",\n",
    "    \"y'alld've\": \"y'all'd've\", \"y'all'dve\": \"y'all'd've\", \"youd\":\n",
    "    \"you'd\", \"youd've\": \"you'd've\", \"you'dve\": \"you'd've\", \"youll\":\n",
    "    \"you'll\", \"youre\": \"you're\", \"youve\": \"you've\"\n",
    "}\n",
    "\n",
    "manual_map = {\n",
    "    'none': '0',\n",
    "    'zero': '0',\n",
    "    'one': '1',\n",
    "    'two': '2',\n",
    "    'three': '3',\n",
    "    'four': '4',\n",
    "    'five': '5',\n",
    "    'six': '6',\n",
    "    'seven': '7',\n",
    "    'eight': '8',\n",
    "    'nine': '9',\n",
    "    'ten': '10'\n",
    "}\n",
    "\n",
    "articles = ['a', 'an', 'the']\n",
    "period_strip = re.compile(\"(?!<=\\d)(\\.)(?!\\d)\")\n",
    "comma_strip = re.compile(\"(\\d)(\\,)(\\d)\")\n",
    "punct = [\n",
    "    ';', r\"/\", '[', ']', '\"', '{', '}',\n",
    "    '(', ')', '=', '+', '\\\\', '_', '-',\n",
    "    '>', '<', '@', '`', ',', '?', '!'\n",
    "]\n",
    "\n",
    "\n",
    "def _process_punctuation(inText):\n",
    "    outText = inText\n",
    "    for p in punct:\n",
    "        if (p + ' ' in inText or ' ' + p in inText) \\\n",
    "        or (re.search(comma_strip, inText) != None):\n",
    "            outText = outText.replace(p, '')\n",
    "        else:\n",
    "            outText = outText.replace(p, ' ')\n",
    "    outText = period_strip.sub(\"\", outText, re.UNICODE)\n",
    "    return outText\n",
    "\n",
    "\n",
    "def _process_digit_article(inText):\n",
    "    outText = []\n",
    "    tempText = inText.lower().split()\n",
    "    for word in tempText:\n",
    "        word = manual_map.setdefault(word, word)\n",
    "        if word not in articles:\n",
    "            outText.append(word)\n",
    "        else:\n",
    "            pass\n",
    "    for wordId, word in enumerate(outText):\n",
    "        if word in contractions:\n",
    "            outText[wordId] = contractions[word]\n",
    "    outText = ' '.join(outText)\n",
    "    return outText\n",
    "\n",
    "\n",
    "def process_a(freq_thr=9):\n",
    "\n",
    "    ta = json.load(open(ta_path), encoding = 'utf-8')['annotations']\n",
    "    va = json.load(open(va_path), encoding = 'utf-8')['annotations']\n",
    "    annos = ta + va\n",
    "\n",
    "    print(\"Calculating the frequency of each multiple choice answer...\")\n",
    "    mca_freqs = {}\n",
    "    for anno in tqdm(annos):\n",
    "        mca = _process_digit_article(_process_punctuation(anno['multiple_choice_answer']))\n",
    "        mca = mca.replace(',', '')\n",
    "        mca_freqs[mca] = mca_freqs.get(mca, 0) + 1\n",
    "\n",
    "    # filter out rare answers\n",
    "    for a, freq in list(mca_freqs.items()):\n",
    "        if freq < freq_thr:\n",
    "            mca_freqs.pop(a)\n",
    "\n",
    "    print(\"Number of answers appear more than {} times: {}\".format(freq_thr - 1, len(mca_freqs)))\n",
    "\n",
    "    # generate answer dictionary\n",
    "    idx2ans = []\n",
    "    ans2idx = {}\n",
    "    for i, a in enumerate(mca_freqs):\n",
    "        idx2ans.append(a)\n",
    "        ans2idx[a] = i\n",
    "\n",
    "    print(\"Generating soft scores...\")\n",
    "    targets = []\n",
    "    for anno in tqdm(annos):\n",
    "        anss = anno['answers']\n",
    "\n",
    "        # calculate individual answer's frequency\n",
    "        ans_freqs = {}\n",
    "        for a in anss:\n",
    "            ans_freqs[a['answer']] = ans_freqs.get(a['answer'], 0) + 1\n",
    "\n",
    "        soft_score = []\n",
    "        for a, freq in ans_freqs.items():\n",
    "            if a in ans2idx:\n",
    "                soft_score.append((a, min(1, freq / 3)))\n",
    "\n",
    "        targets.append({\n",
    "            'question_id': anno['question_id'],\n",
    "            'image_id': anno['image_id'],\n",
    "            'answer': soft_score    # [(ans1, score1), (ans2, score2), ...]\n",
    "        })\n",
    "\n",
    "    pickle.dump([idx2ans, ans2idx], open(os.path.join('data', 'dict_ans.pkl'), 'wb'))\n",
    "    return targets\n",
    "\n",
    "\n",
    "def _tokenize(tbt):\n",
    "    tbt = tbt.lower().replace(',', '').replace('?', '')\n",
    "    return word_tokenize(tbt)\n",
    "\n",
    "\n",
    "def process_qa(targets, max_words=14):\n",
    "\n",
    "    print(\"Merging QAs...\")\n",
    "    idx2word = []\n",
    "    word2idx = {}\n",
    "\n",
    "    tq = json.load(open(tq_path))['questions']\n",
    "    vq = json.load(open(vq_path))['questions']\n",
    "    qs = tq + vq\n",
    "    qas = []\n",
    "    for i, q in enumerate(tqdm(qs)):\n",
    "        tokens = _tokenize(q['question'])\n",
    "        for t in tokens:\n",
    "            if not t in word2idx:\n",
    "                idx2word.append(t)\n",
    "                word2idx[t] = len(idx2word) - 1\n",
    "\n",
    "        assert q['question_id'] == targets[i]['question_id'],\\\n",
    "                \"Question ID doesn't match ({}: {})\".format(q['question_id'], targets[i]['question_id'])\n",
    "\n",
    "        qas.append({\n",
    "            'image_id': q['image_id'],\n",
    "            'question': q['question'],\n",
    "            'question_id': q['question_id'],\n",
    "            'question_toked': tokens,\n",
    "            'answer': targets[i]['answer']\n",
    "        })\n",
    "\n",
    "    pickle.dump([idx2word, word2idx], open(os.path.join('data', 'dict_q.pkl'), 'wb'))\n",
    "    pickle.dump(qas[:len(tq)], open(os.path.join('data', 'train_qa.pkl'), 'wb'))\n",
    "    pickle.dump(qas[len(tq):], open(os.path.join('data', 'val_qa.pkl'), 'wb'))\n",
    "    return idx2word\n",
    "\n",
    "\n",
    "def process_wemb(idx2word):\n",
    "    print(\"Generating pretrained word embedding weights...\")\n",
    "    word2emb = {}\n",
    "    emb_dim = int(glove_path.split('.')[-2].split('d')[0])\n",
    "    with open(glove_path, encoding = 'utf-8') as f:\n",
    "        for entry in f:\n",
    "            vals = entry.split(' ')\n",
    "            word = vals[0]\n",
    "            word2emb[word] = np.asarray(vals[1:], dtype=np.float32)\n",
    "\n",
    "    pretrained_weights = np.zeros((len(idx2word), emb_dim), dtype=np.float32)\n",
    "    for idx, word in enumerate(idx2word):\n",
    "        if word not in word2emb:\n",
    "            continue\n",
    "        pretrained_weights[idx] = word2emb[word]\n",
    "\n",
    "    np.save(os.path.join('data', 'glove_pretrained_{}.npy'.format(emb_dim)), pretrained_weights)\n",
    "\n",
    "\n",
    "def process_vfeats():\n",
    "    FIELDNAMES = ['image_id', 'image_w', 'image_h', 'num_boxes', 'boxes', 'features']\n",
    "    tq = json.load(open(tq_path))['questions']\n",
    "    vq = json.load(open(vq_path))['questions']\n",
    "    tids = set([q['image_id'] for q in tq])\n",
    "    vids = set([q['image_id'] for q in vq])\n",
    "    # tids_ = list(tids)[:20000]\n",
    "    # vids_ = list(vids)[:20000]\n",
    "\n",
    "    print(\"Reading tsv, total iterations: {}\".format(len(tids)+len(vids)))\n",
    "    tvfeats = {}\n",
    "    vvfeats = {}\n",
    "    with open(vfeats_path, encoding='utf-8') as tsv_in_file:\n",
    "        reader = csv.DictReader(tsv_in_file, delimiter='\\t', fieldnames=FIELDNAMES)\n",
    "        for i, item in enumerate(tqdm(reader)):\n",
    "            image_id = int(item['image_id'])\n",
    "            feats = np.frombuffer(base64.b64decode(item['features']), \n",
    "                dtype=np.float32).reshape((int(item['num_boxes']), -1))\n",
    "\n",
    "            if image_id in tids:\n",
    "                tvfeats[image_id] = feats\n",
    "            elif image_id in vids:\n",
    "                vvfeats[image_id] = feats\n",
    "            else:\n",
    "                raise ValueError(\"Image_id: {} not in training or validation set\".format(image_id))\n",
    "\n",
    "    print(\"Converting tsv to pickle... This will take a while\")\n",
    "    return tvfeats, vvfeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the frequency of each multiple choice answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 658111/658111 [00:18<00:00, 36272.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of answers appear more than 8 times: 3129\n",
      "Generating soft scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 658111/658111 [00:04<00:00, 147960.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging QAs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 658111/658111 [00:50<00:00, 13126.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating pretrained word embedding weights...\n",
      "Reading tsv, total iterations: 123287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123287it [20:57, 98.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting tsv to pickle... This will take a while\n"
     ]
    }
   ],
   "source": [
    "targets = process_a()\n",
    "idx2word = process_qa(targets)\n",
    "process_wemb(idx2word)\n",
    "tvfeats, vvfeats = process_vfeats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82783, 40504)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tvfeats), len(vvfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(tvfeats, open(os.path.join('data', 'train_vfeats.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(vvfeats, open(os.path.join('data', 'val_vfeats.pkl'), 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
