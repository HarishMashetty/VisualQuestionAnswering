{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vqa",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "5BlZL4h7HOrN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#! git clone https://github.com/HarishMashetty/VisualQuestionAnswering.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eyir-jrwDTJ1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!chmod 777 /content/VisualQuestionAnswering/vqa-winner-cvprw-2017/scripts/download_extract.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c6tSXX_pijqz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!/content/VisualQuestionAnswering/vqa-winner-cvprw-2017/scripts/download_extract.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4SbozVsqdr7h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!python /content/VisualQuestionAnswering/vqa-winner-cvprw-2017/scripts/preproc.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TUHjINjosAc4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#!/content/VisualQuestionAnswering/vqa-winner-cvprw-2017/scripts/download_extract.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WWNmPBsj-3pp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Check resources available on Colab GPU**"
      ]
    },
    {
      "metadata": {
        "id": "-q3OZ4n187ko",
        "colab_type": "code",
        "outputId": "24b6ccec-1c98-4034-b054-eb51ddabe269",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.9 GB  | Proc size: 142.8 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2fRj9R_w90Gk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TVGFEmcLA39i",
        "colab_type": "code",
        "outputId": "c8aff53b-64d6-4e39-eb0c-c42c55d70a97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print (\"CUDA is available\")\n",
        "else:\n",
        "  raise Exception('CUDA not found')\n",
        "\n",
        "torch.cuda.manual_seed(0)\n",
        "glove_embedding_dimension=300\n",
        "\n",
        "\n",
        "# Data loader settings\n",
        "pin_memory =True\n",
        "n_workers = 4\n",
        "train_batch_size=256\n",
        "\n",
        "#Model architecture settings\n",
        "hidden_dimension=512\n",
        "\n",
        "# Training settings\n",
        "max_epochs =10\n",
        "\n",
        "\n",
        "globe_pretrained_file=\"/content/data/glove_pretrained_300.npy\"\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xjpXQJbVRrRX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Run Model"
      ]
    },
    {
      "metadata": {
        "id": "dhmBkwWzA5xc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load dictionary files\n",
        "\n",
        "# dictionary of all question word tokens, both training and validation. example idx2word = ['does', 'the', 'guy', 'have', 'a', 'tattoo' ...]\n",
        "idx2word, word2idx = pickle.load(open('/content/data/dict_q.pkl', 'rb') )\n",
        "\n",
        "# dictionary of all answers, both training and validation. example idx2ans = ['blue','cell phone','blue and white'....]\n",
        "idx2ans, ans2idx = pickle.load(open('/content/data/dict_ans.pkl', 'rb') )\n",
        "\n",
        "vocab_size=len(idx2word)\n",
        "n_classes=len(idx2ans)\n",
        "\n",
        "\n",
        "# load preprocessed training files\n",
        "question_answers = pickle.load(open('/content/data/train_qa.pkl', 'rb') )\n",
        "visual_features = joblib.load(open('/content/data/train_vfeats.pkl', 'rb') ) \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fVNRhrL9FK4l",
        "colab_type": "code",
        "outputId": "0d4a224c-c590-4184-9c40-4741929c092e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "n_classes\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "475"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "sV9qSKOJznFi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Create Dataset class , which feeds data to Data loader**"
      ]
    },
    {
      "metadata": {
        "id": "_Jsqo6kRz_lw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_v12Tg-JtxT5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils import data\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class vqa_dataset(data.Dataset):\n",
        "\n",
        "  def __init__(self, question_answers, visual_features,vocab_size,word2idx,n_classes):\n",
        "    self.question_answers = question_answers\n",
        "    self.visual_features = visual_features\n",
        "    self.data_list = []\n",
        "    \n",
        "    max_words=14\n",
        "    \n",
        "    for q in question_answers:\n",
        "\n",
        "      # build question vector\n",
        "      # question tokened = ['does', 'the', 'guy', 'have', 'a', 'tattoo']\n",
        "      # question vector, dictionaty index, which is used to get glove embeddings using nn.Embedding as a lookup table\n",
        "      # question vector = [ 7,  8, 38, 39,  16, 23,  0,  0,  0,  0,  0,  0,  0,  0] , indices corresponding to\n",
        "      #                   the words in dictionary, using these indices we do a lookup in pretrained golve dict\n",
        "  \n",
        "      question_vector = np.zeros(max_words, dtype=np.int64)\n",
        "\n",
        "      for i, question_token in enumerate(q['question_toked']):\n",
        "          if question_token in word2idx and i < 14:\n",
        "            question_vector[i] = word2idx[question_token]\n",
        "\n",
        "      # build answer vector\n",
        "      # answer answer tuple input = ('yes', 1)\n",
        "      # answer vector = \n",
        "\n",
        "      answer_vector = np.zeros(n_classes, dtype=np.float32)\n",
        "      for answer, score in q['answer']:\n",
        "        answer_vector[ans2idx[answer]] = score\n",
        "\n",
        "      if q['image_id'] in visual_features:\n",
        "        self.data_list.append({ 'image': visual_features[q['image_id']], 'question': question_vector,'answer': answer_vector})\n",
        "        \n",
        "\n",
        "  def __len__(self):\n",
        "    'Denotes the total number of samples'\n",
        "    return len(self.question_answers)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    'Generates one sample of data'\n",
        "    # Select sample\n",
        "    selected = self.data_list[index]\n",
        "    \n",
        "    return selected['image'], selected['question'], selected['answer']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p1Yd0IkLtYv_",
        "colab_type": "code",
        "outputId": "888f7d77-f428-4aaf-b903-d71e82a02f6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "training_set = vqa_dataset(question_answers, visual_features,vocab_size,word2idx,n_classes)\n",
        "v,q,a=training_set.__getitem__(0)\n",
        "\n",
        "\n",
        "a.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(475,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "metadata": {
        "id": "BEML7gf-jD26",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VQA_Model(nn.Module):\n",
        "\n",
        "\n",
        "  # constructor\n",
        "  def __init__(self, vocab_size, globe_pretrained_file, hidden_dimension, num_classes,embedding_dimension):\n",
        "    \n",
        "    super(VQA_Model, self).__init__()    \n",
        "    \n",
        "    self.hidden_dimension=hidden_dimension\n",
        "    \n",
        "    # Question embedding\n",
        "    self.nn_emedding = nn.Embedding(vocab_size + 1,embedding_dimension )\n",
        "    self.gru = nn.GRU(embedding_dimension, hidden_dimension)\n",
        "\n",
        "    \n",
        "    # treat nn.Embedding as a lookup table where the key is the word index and # the value is the corresponding word vector.\n",
        "    #Specify the #size of the lookup table, and initialize the word vectors yourself.\n",
        "\n",
        "\n",
        "    # vocab size + 1\n",
        "    pretrained_glove_weights = np.zeros((vocab_size + 1, embedding_dimension), dtype=np.float32)\n",
        "\n",
        "    # last row of the ndarray is kept 0\n",
        "    pretrained_glove_weights[:vocab_size] = np.load(globe_pretrained_file)\n",
        "\n",
        "    # copy weights to nn_embedding, last row of the weights nn_emedding.weight is 0 vector\n",
        "    self.nn_emedding.weight.data.copy_(torch.from_numpy(pretrained_glove_weights))\n",
        "    \n",
        "\n",
        "    self.linear_layer_scalar_attention = nn.Linear(hidden_dimension, 1)\n",
        "\n",
        "    # classifier\n",
        "\n",
        "    self.classifier_linear = nn.Linear(hidden_dimension, num_classes)\n",
        "    #self.clf_dropout = nn.Dropout(0.5, inplace=True)\n",
        "\n",
        "    # Linear layers for, non linear activation\n",
        "\n",
        "    # gated tanh for top down attention\n",
        "    # to calculate fa([vi; q])\n",
        "    self.linear_layer_fa_s = nn.Linear(2048 + hidden_dimension, hidden_dimension)\n",
        "    self.linear_layer_fa_t = nn.Linear(2048 + hidden_dimension, hidden_dimension)\n",
        "    \n",
        "\n",
        "    \n",
        "    # to calculate fq(q)\n",
        "    self.linear_layer_q_s = nn.Linear(hidden_dimension, hidden_dimension)\n",
        "    self.linear_layer_q_t = nn.Linear(hidden_dimension, hidden_dimension)\n",
        "    \n",
        "    # to calculate fv(^v)\n",
        "    self.linear_layer_v_hat_s = nn.Linear(2048, hidden_dimension)\n",
        "    self.linear_layer_v_hat_t = nn.Linear(2048, hidden_dimension)\n",
        "    \n",
        "    # to calculate fo(h)\n",
        "    self.linear_layer_o_s = nn.Linear(hidden_dimension, hidden_dimension)\n",
        "    self.linear_layer_o_t = nn.Linear(hidden_dimension, hidden_dimension)\n",
        "    \n",
        "\n",
        "  def forward(self, image, question):\n",
        "    \n",
        "    # Shape (batch_size, 14 question words, 300 size embedding for each word)\n",
        "    question_embedding = self.nn_emedding(question)  \n",
        "    \n",
        "    # gru encoding shape (14 , batch_size, hidden dimension)\n",
        "    gru_encoding, gru_hidden = self.gru(question_embedding.permute(1, 0, 2))\n",
        "    \n",
        "    # question encoding (batch_size, hidden dimension)\n",
        "    question_encoding = gru_encoding[-1]\n",
        "    question_reshape = question_encoding.repeat(1, 36).view(-1, 36, self.hidden_dimension)\n",
        "\n",
        "    \n",
        "    # Implement a-i (scalar attention)  = wa * fa ( concatenation of [v-i; q] )\n",
        "    image = F.normalize(image, -1)\n",
        "    \n",
        "    #torch.cat(tensors, dim) , dim = dimension over which the tensors are concatenated\n",
        "    concatenation= torch.cat((image,question_reshape),-1)\n",
        "    \n",
        "    concatenation = torch.mul(torch.tanh(self.linear_layer_fa_s(concatenation)), torch.sigmoid(self.linear_layer_fa_s(concatenation)))\n",
        "    \n",
        "    \n",
        "    scalar_attention = self.linear_layer_scalar_attention(concatenation)    \n",
        "    \n",
        "    # alpha = softmax (a)\n",
        "    \n",
        "    scalar_attention_softmax = F.softmax(scalar_attention.squeeze(), dim=1)\n",
        "        \n",
        "    # v hat (attended image) = comvex combination of (alpha-i * v-i)\n",
        "\n",
        "    attended_image = torch.bmm(scalar_attention_softmax.unsqueeze(1), image).squeeze()\n",
        "\n",
        "    # Multimodal fusion h = fq(q) * fv(v hat)\n",
        "    \n",
        "    f_v_hat  = torch.mul(torch.tanh(self.linear_layer_v_hat_t(attended_image)), torch.sigmoid(self.linear_layer_v_hat_s(attended_image)))\n",
        "    f_q  = torch.mul(torch.tanh(self.linear_layer_q_t(question_encoding)), torch.sigmoid(self.linear_layer_q_s(question_encoding)))\n",
        "\n",
        "    multimodal_fusion =  torch.mul(f_q, f_v_hat)\n",
        "    \n",
        "    # s = sigmoid (wo * fo(h) )\n",
        "    s = self.classifier_linear(torch.mul(torch.tanh(self.linear_layer_o_t(multimodal_fusion)), torch.sigmoid(self.linear_layer_o_s(multimodal_fusion))))\n",
        "    \n",
        "    return s\n",
        "\n",
        "    \n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MEPjV5m_vz5m",
        "colab_type": "code",
        "outputId": "89050d0a-fab7-43e9-90a3-b1d41c02d5f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "print (\"Question Vocabulary size:\" + str(vocab_size))\n",
        "print (\"hidden layer dimension:\" + str(hidden_dimension))\n",
        "print (\"number of classes for classification:\" + str(n_classes))\n",
        "\n",
        "model = VQA_Model(vocab_size, globe_pretrained_file, hidden_dimension, n_classes,glove_embedding_dimension)\n",
        "model = nn.DataParallel(model).to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question Vocabulary size:7069\n",
            "hidden layer dimension:512\n",
            "number of classes for classification:475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6gEUpgPafG0J",
        "colab_type": "code",
        "outputId": "a3f35c61-8847-43eb-f9bd-632c580e9df5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "input = torch.randn(3, requires_grad=True)\n",
        "target = torch.empty(3).random_(2)\n",
        "loss = F.binary_cross_entropy_with_logits(input, target)\n",
        "print (loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.0193, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2xaDykcqh8R3",
        "colab_type": "code",
        "outputId": "1d1898b2-63d0-415a-cf8a-a0cb5c82a902",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3, 1, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "umYjYdIXl3n6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Generators\n",
        "training_set = vqa_dataset(question_answers, visual_features,vocab_size,word2idx,n_classes)\n",
        "training_generator = data.DataLoader(training_set,train_batch_size, shuffle=True, num_workers =n_workers, pin_memory=pin_memory)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VdsxgPiN_wDX",
        "colab_type": "code",
        "outputId": "c25196ce-1c7f-4ec3-ed8c-b8c3a69eba5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question Vocabulary size:7069\n",
            "hidden layer dimension:512\n",
            "number of classes for classification:475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ItBTRKz2sYxY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_score(logits, labels):\n",
        "    logits = torch.max(logits, 1)[1].data\n",
        "    if torch.cuda.is_available():\n",
        "        one_hots = torch.zeros(*labels.size()).cuda()\n",
        "    else:\n",
        "        one_hots = torch.zeros(*labels.size())\n",
        "    one_hots.scatter_(1, logits.view(-1, 1), 1)\n",
        "    score = (one_hots * labels)\n",
        "    return score.cpu().numpy().sum() / logits.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uexBqRKP0vW9",
        "colab_type": "code",
        "outputId": "dba0f1ef-2e40-4747-8817-b1ec7ce7e378",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "cell_type": "code",
      "source": [
        "optim = torch.optim.Adamax(model.parameters())\n",
        "\n",
        "# Loop over epochs\n",
        "for epoch in range(max_epochs):\n",
        "  for local_image, local_question,local_answer in training_generator:\n",
        "        # Transfer to GPU\n",
        "        local_image, local_question,local_answer = local_image.to(device), local_question.to(device),local_answer.to(device)\n",
        "        \n",
        "        logits = model(local_image, local_question)\n",
        "        \n",
        "        #print (model)\n",
        "        #print (epoch,len(local_answer))\n",
        "        \n",
        "        #print (logits)\n",
        "        \n",
        "        loss = F.binary_cross_entropy_with_logits(logits, local_answer) * local_answer.size(1)\n",
        "\n",
        "\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        #nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
        "        optim.step()\n",
        "\n",
        "        score = compute_score(logits, local_answer)\n",
        "  print (\"Epoch : \" + str(1) + \", score :\" + str (score) )\n",
        "        \n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : epoch, score :0.31286550823010895\n",
            "Epoch : epoch, score :0.35526312443248015\n",
            "Epoch : epoch, score :0.40935670283802766\n",
            "Epoch : epoch, score :0.42543859649122806\n",
            "Epoch : epoch, score :0.4356725257739686\n",
            "Epoch : epoch, score :0.42543859649122806\n",
            "Epoch : epoch, score :0.45175445288942573\n",
            "Epoch : epoch, score :0.5087719298245614\n",
            "Epoch : epoch, score :0.47222221106813667\n",
            "Epoch : epoch, score :0.49269010309587447\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rFge8OvU0ymK",
        "colab_type": "code",
        "outputId": "01d6b3a1-e905-4b68-8865-1da98946d4eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "metadata": {
        "id": "bVRl0lSXh4jp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Build Model**"
      ]
    },
    {
      "metadata": {
        "id": "AT_S_fJkh2bc",
        "colab_type": "code",
        "outputId": "4f880ad6-98d7-4964-b016-ceb939ead32c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1310
        }
      },
      "cell_type": "code",
      "source": [
        " for local_image, local_question,local_answer in training_generator:\n",
        "    print (len(local_answer))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-383b4735214f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mlocal_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_question\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlocal_answer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m    \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_answer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMP_STATUS_CHECK_INTERVAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "9nji9-23MlA_",
        "colab_type": "code",
        "outputId": "3fad9e44-9909-4f36-8b96-6fa0139384f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "# treat nn.Embedding as a lookup table where the key is the word index and # the value is the corresponding word vector.\n",
        "#Specify the #size of the lookup table, and initialize the word vectors yourself.\n",
        "\n",
        "pretrained_glove_weights = np.zeros((vocab_size + 1, glove_embedding_dimension), dtype=np.float32)\n",
        "\n",
        "# last row of the ndarray is kept 0\n",
        "pretrained_glove_weights[:vocab_size] = np.load(globe_pretrained_file)\n",
        "\n",
        "# copy weights to nn_embedding, last row of the weights nn_emedding.weight is 0 vector\n",
        "nn_emedding.weight.data.copy_(torch.from_numpy(pretrained_glove_weights))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1570,  0.2190, -0.1663,  ..., -0.1872,  0.2802,  0.2936],\n",
              "        [ 0.0466,  0.2132, -0.0074,  ...,  0.0091, -0.2099,  0.0539],\n",
              "        [-0.3004, -0.0474, -0.1218,  ...,  0.5475, -0.0809,  0.3682],\n",
              "        ...,\n",
              "        [-0.1160, -0.1870, -0.4214,  ...,  0.5555,  0.2341, -0.0101],\n",
              "        [-0.4783, -0.4500, -0.0590,  ...,  0.4060,  0.5669,  0.3657],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "metadata": {
        "id": "jpbCDDlLhvSB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xg02732BenNy",
        "colab_type": "code",
        "outputId": "dd33ec55-f711-4c78-e179-018bdc446cb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.1570,  0.2190, -0.1663,  ..., -0.1872,  0.2802,  0.2936],\n",
              "        [ 0.0466,  0.2132, -0.0074,  ...,  0.0091, -0.2099,  0.0539],\n",
              "        [-0.3004, -0.0474, -0.1218,  ...,  0.5475, -0.0809,  0.3682],\n",
              "        ...,\n",
              "        [-0.1160, -0.1870, -0.4214,  ...,  0.5555,  0.2341, -0.0101],\n",
              "        [-0.4783, -0.4500, -0.0590,  ...,  0.4060,  0.5669,  0.3657],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "metadata": {
        "id": "d8Fr3CcNeIC5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "38G5zNEtI9h4",
        "colab_type": "code",
        "outputId": "bdf399a2-8cff-46a9-f627-e6bb48b5d8af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "question_answers[0]['question_toked']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['does', 'the', 'guy', 'have', 'a', 'tattoo']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "Oz-mMe_NMsBv",
        "colab_type": "code",
        "outputId": "49e7a77a-b63b-406f-a3b8-453354326bbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17368
        }
      },
      "cell_type": "code",
      "source": [
        "idx2word "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['does',\n",
              " 'the',\n",
              " 'guy',\n",
              " 'have',\n",
              " 'a',\n",
              " 'tattoo',\n",
              " 'what',\n",
              " 'is',\n",
              " 'this',\n",
              " 'man',\n",
              " 'riding',\n",
              " 'on',\n",
              " 'how',\n",
              " 'many',\n",
              " 'tattoos',\n",
              " 'can',\n",
              " 'be',\n",
              " 'seen',\n",
              " \"'s\",\n",
              " 'body',\n",
              " 'color',\n",
              " 'his',\n",
              " 'hat',\n",
              " 'visor',\n",
              " 'providing',\n",
              " 'face',\n",
              " 'enough',\n",
              " 'protection',\n",
              " 'plane',\n",
              " 'landing',\n",
              " 'shape',\n",
              " 'are',\n",
              " 'windows',\n",
              " 'side',\n",
              " 'of',\n",
              " 'taking',\n",
              " 'off',\n",
              " 'water',\n",
              " 'photo',\n",
              " 'outdoors',\n",
              " 'birds',\n",
              " 'in',\n",
              " 'sink',\n",
              " 'planes',\n",
              " 'tail',\n",
              " '4',\n",
              " 'colors',\n",
              " 'last',\n",
              " 'letter',\n",
              " 'over',\n",
              " 'these',\n",
              " 'wings',\n",
              " 'strong',\n",
              " 'sign',\n",
              " 'would',\n",
              " 'you',\n",
              " 'able',\n",
              " 'to',\n",
              " 'find',\n",
              " 'mosque',\n",
              " 'city',\n",
              " 'language',\n",
              " 'written',\n",
              " 'printed',\n",
              " 'orange',\n",
              " 'name',\n",
              " 'hotel',\n",
              " 'vehicles',\n",
              " 'shown',\n",
              " 'mode',\n",
              " 'transportation',\n",
              " 'pictured',\n",
              " 'small',\n",
              " 'town',\n",
              " 'front',\n",
              " 'bus',\n",
              " 'say',\n",
              " 'at',\n",
              " 'top',\n",
              " 'featured',\n",
              " 'picture',\n",
              " 'typical',\n",
              " 'school',\n",
              " 'latest',\n",
              " 'make',\n",
              " 'and',\n",
              " 'model',\n",
              " 'there',\n",
              " 'any',\n",
              " 'building',\n",
              " 'area',\n",
              " 'church',\n",
              " 'sunny',\n",
              " 'or',\n",
              " 'overcast',\n",
              " 'snow',\n",
              " 'roof',\n",
              " 'shingles',\n",
              " 'symbol',\n",
              " 'sits',\n",
              " 'atop',\n",
              " 'tower',\n",
              " 'clock',\n",
              " 'woman',\n",
              " 'wearing',\n",
              " 'dress',\n",
              " 'they',\n",
              " 'playing',\n",
              " 'game',\n",
              " 'will',\n",
              " 'bent',\n",
              " 'person',\n",
              " 'fall',\n",
              " 'forward',\n",
              " 'blurry',\n",
              " 'get',\n",
              " 'ticket',\n",
              " 'if',\n",
              " 'disobeyed',\n",
              " 'red',\n",
              " 'stop',\n",
              " 'wooden',\n",
              " 'pole',\n",
              " 'says',\n",
              " 'see',\n",
              " 'sun',\n",
              " 'floors',\n",
              " 'do',\n",
              " 'think',\n",
              " 'highest',\n",
              " 'has',\n",
              " 'ever',\n",
              " 'been',\n",
              " 'intersection',\n",
              " 'street',\n",
              " 'that',\n",
              " 'starts',\n",
              " 'with',\n",
              " 'men',\n",
              " 'hats',\n",
              " 'right',\n",
              " 'look',\n",
              " 'happy',\n",
              " 'coworkers',\n",
              " 'best',\n",
              " 'friends',\n",
              " 'glass',\n",
              " 'lower',\n",
              " 'wall',\n",
              " 'made',\n",
              " 'out',\n",
              " 'trying',\n",
              " 'fix',\n",
              " 'train',\n",
              " 'locomotive',\n",
              " 'black',\n",
              " 'she',\n",
              " 'bonnet',\n",
              " 'was',\n",
              " 'collar',\n",
              " 'treated',\n",
              " 'starch',\n",
              " 'pattern',\n",
              " 'number',\n",
              " 'lifeguard',\n",
              " 'chair',\n",
              " 'girl',\n",
              " 'left',\n",
              " 'bikini',\n",
              " 'bottom',\n",
              " 'green',\n",
              " 'umbrellas',\n",
              " 'people',\n",
              " 'for',\n",
              " 'commercial',\n",
              " 'flights',\n",
              " 'mountain',\n",
              " 'tops',\n",
              " 'tennis',\n",
              " 'net',\n",
              " 'purple',\n",
              " 'fence',\n",
              " 'catch',\n",
              " 'ball',\n",
              " 'serving',\n",
              " 'often',\n",
              " 'take',\n",
              " 'selfies',\n",
              " 'like',\n",
              " 'looking',\n",
              " 'window',\n",
              " 'car',\n",
              " 'showing',\n",
              " 'mirror',\n",
              " 'reflection',\n",
              " 'doing',\n",
              " 'cell',\n",
              " 'phone',\n",
              " 'texting',\n",
              " 'legal',\n",
              " 'when',\n",
              " 'driving',\n",
              " 'cars',\n",
              " 'hanging',\n",
              " 'ketchup',\n",
              " 'food',\n",
              " 'flavor',\n",
              " 'beverage',\n",
              " 'napkin',\n",
              " 'healthy',\n",
              " 'meal',\n",
              " 'racket',\n",
              " 'brand',\n",
              " 'player',\n",
              " 'jumping',\n",
              " 'air',\n",
              " 'facial',\n",
              " 'hair',\n",
              " 'stripe',\n",
              " 'players',\n",
              " 'shorts',\n",
              " 'race',\n",
              " 'two',\n",
              " 'objects',\n",
              " 'fly',\n",
              " 'by',\n",
              " 'companies',\n",
              " 'flying',\n",
              " 'bird',\n",
              " 'hall',\n",
              " 'kind',\n",
              " 'tent',\n",
              " 'it',\n",
              " 'raining',\n",
              " 'shining',\n",
              " 'tires',\n",
              " 'directly',\n",
              " 'behind',\n",
              " 'back',\n",
              " 'district',\n",
              " 'lights',\n",
              " 'busses',\n",
              " 'buses',\n",
              " 'type',\n",
              " 'flower',\n",
              " 'vase',\n",
              " 'table',\n",
              " 'flowers',\n",
              " 'standing',\n",
              " 'up',\n",
              " 'pens',\n",
              " 'him',\n",
              " 'young',\n",
              " 'an',\n",
              " 'old',\n",
              " 'tie',\n",
              " 'elevator',\n",
              " 'himself',\n",
              " 'seems',\n",
              " 'caught',\n",
              " 'fire',\n",
              " 'emergency',\n",
              " 'which',\n",
              " 'direction',\n",
              " 'turn',\n",
              " 'lane',\n",
              " 'going',\n",
              " 'bristles',\n",
              " 'toothbrush',\n",
              " 'farthest',\n",
              " 'from',\n",
              " 'camera',\n",
              " 'one',\n",
              " 'more',\n",
              " 'used',\n",
              " 'than',\n",
              " 'other',\n",
              " 'toothbrushes',\n",
              " 'could',\n",
              " 'brushes',\n",
              " 'charging',\n",
              " 'bowl',\n",
              " 'middle',\n",
              " 'batter',\n",
              " 'displayed',\n",
              " 'being',\n",
              " 'played',\n",
              " 'shoe',\n",
              " 'cat',\n",
              " 'eat',\n",
              " 'who',\n",
              " 'sunglasses',\n",
              " 'sitting',\n",
              " 'vest',\n",
              " 'circular',\n",
              " 'structure',\n",
              " 'group',\n",
              " 'snowboarders',\n",
              " 'where',\n",
              " 'mean',\n",
              " 'all',\n",
              " 'facing',\n",
              " 'same',\n",
              " 'propellers',\n",
              " 'sky',\n",
              " 'photographer',\n",
              " 'took',\n",
              " 'image',\n",
              " 'jet',\n",
              " 'orientated',\n",
              " 'large',\n",
              " 'crowd',\n",
              " 'why',\n",
              " 'police',\n",
              " 'protesting',\n",
              " 'something',\n",
              " 'shirts',\n",
              " 'team',\n",
              " 'open',\n",
              " 'holding',\n",
              " 'chairs',\n",
              " 'laptop',\n",
              " 'cheap',\n",
              " 'house',\n",
              " 'live',\n",
              " 'might',\n",
              " 'here',\n",
              " 'room',\n",
              " 'bike',\n",
              " 'connected',\n",
              " 'markings',\n",
              " 'pavement',\n",
              " 'head',\n",
              " 'bikes',\n",
              " 'moving',\n",
              " 'designed',\n",
              " 'motorcycles',\n",
              " 'customized',\n",
              " 'her',\n",
              " 'outfit',\n",
              " 'impractical',\n",
              " 'activity',\n",
              " 'faster',\n",
              " 'whose',\n",
              " 'lady',\n",
              " 'neon',\n",
              " 'read',\n",
              " 'square',\n",
              " 'colorful',\n",
              " 'streets',\n",
              " 'airline',\n",
              " 'happens',\n",
              " 'let',\n",
              " 'go',\n",
              " 'rope',\n",
              " 'date',\n",
              " 'arm',\n",
              " 'court',\n",
              " 'about',\n",
              " 'time',\n",
              " 'day',\n",
              " 'taken',\n",
              " 'dressed',\n",
              " 'bee',\n",
              " 'hands',\n",
              " 'sport',\n",
              " 'board',\n",
              " 'sand',\n",
              " 'waves',\n",
              " 'ocean',\n",
              " 'passenger',\n",
              " 'a-10',\n",
              " 'warthog',\n",
              " 'traffic',\n",
              " 'cone',\n",
              " 'blue',\n",
              " 'f',\n",
              " '16',\n",
              " 'wheels',\n",
              " 'historical',\n",
              " 'restroom',\n",
              " 'expensive',\n",
              " 'restaurant',\n",
              " 'accommodate',\n",
              " 'silver',\n",
              " 'box',\n",
              " 'below',\n",
              " 'skis',\n",
              " 'weather',\n",
              " 'appear',\n",
              " 'snowy',\n",
              " 'fast',\n",
              " 'geometric',\n",
              " 'design',\n",
              " 'between',\n",
              " 'feet',\n",
              " 'button',\n",
              " 'again',\n",
              " 'hand',\n",
              " 'paper',\n",
              " 'background',\n",
              " 'television',\n",
              " 'cold',\n",
              " 'surfer',\n",
              " 'he',\n",
              " 'wetsuit',\n",
              " 'high',\n",
              " 'formed',\n",
              " 'surfboard',\n",
              " 'leaning',\n",
              " 'professional',\n",
              " 'kitchen',\n",
              " 'making',\n",
              " 'gyros',\n",
              " 'skateboard',\n",
              " 'enjoying',\n",
              " 'condition',\n",
              " 'transporter',\n",
              " 'land',\n",
              " 'night',\n",
              " 'child',\n",
              " 'ankle',\n",
              " 'socks',\n",
              " 'scene',\n",
              " 'both',\n",
              " 'sit',\n",
              " 'char',\n",
              " 'alcoholic',\n",
              " 'drinks',\n",
              " 'books',\n",
              " 'shelves',\n",
              " 'visible',\n",
              " 'full',\n",
              " 'grown',\n",
              " 'climbing',\n",
              " 'parade',\n",
              " 'horses',\n",
              " 'skating',\n",
              " 'skateboarder',\n",
              " 'safety',\n",
              " 'gear',\n",
              " 'someone',\n",
              " 'antique',\n",
              " 'moped',\n",
              " 'palm',\n",
              " 'tree',\n",
              " 'trunks',\n",
              " 'painted',\n",
              " 'happening',\n",
              " 'stuffed',\n",
              " 'bear',\n",
              " 'animal',\n",
              " 'sentimental',\n",
              " 'inside',\n",
              " 'ipod',\n",
              " 'devices',\n",
              " 'mess',\n",
              " 'preparing',\n",
              " 'helmet',\n",
              " 'organic',\n",
              " 'store',\n",
              " \"n't\",\n",
              " 'cucumbers',\n",
              " 'bit',\n",
              " 'snack',\n",
              " 'bottles',\n",
              " 'above',\n",
              " 'produce',\n",
              " 'fruit',\n",
              " 'artichokes',\n",
              " 'signs',\n",
              " 'american',\n",
              " 'shopping',\n",
              " 'mall',\n",
              " 'relation',\n",
              " 'figure',\n",
              " 'chinese',\n",
              " 'character',\n",
              " 'happened',\n",
              " 'still',\n",
              " 'gender',\n",
              " 'next',\n",
              " 'storage',\n",
              " 'container',\n",
              " 'new',\n",
              " 'boys',\n",
              " 'knees',\n",
              " 'currently',\n",
              " 'motion',\n",
              " 'boy',\n",
              " 'wear',\n",
              " 'protective',\n",
              " 'trendy',\n",
              " 'jeans',\n",
              " 'normal',\n",
              " 'clothes',\n",
              " 'vehicle',\n",
              " 'did',\n",
              " 'place',\n",
              " 'disk',\n",
              " 'drum',\n",
              " 'brakes',\n",
              " 'rims',\n",
              " 'shower',\n",
              " 'toilet',\n",
              " 'faucets',\n",
              " 'bathroom',\n",
              " 'stove',\n",
              " 'towels',\n",
              " 'kleenex',\n",
              " 'sleep',\n",
              " 'animals',\n",
              " 'ground',\n",
              " 'swimsuit',\n",
              " 'deep',\n",
              " 'concerned',\n",
              " 'sharks',\n",
              " 'female',\n",
              " 'women',\n",
              " 'piece',\n",
              " 'attractive',\n",
              " 'stand',\n",
              " 'male',\n",
              " 'bending',\n",
              " 'down',\n",
              " 'audience',\n",
              " 'wheelchairs',\n",
              " 'sports',\n",
              " 'advertised',\n",
              " 'clothing',\n",
              " 'mannequins',\n",
              " 'balls',\n",
              " 'names',\n",
              " 'popular',\n",
              " 'video',\n",
              " 'yield',\n",
              " 'tall',\n",
              " 'long',\n",
              " 'sleeved',\n",
              " 'shirt',\n",
              " 'guys',\n",
              " 'trousers',\n",
              " 'proper',\n",
              " 'work',\n",
              " 'straight',\n",
              " 'ahead',\n",
              " 'issues',\n",
              " 'tuxedo',\n",
              " 'position',\n",
              " 'robot',\n",
              " 'bat',\n",
              " 'real',\n",
              " 'umbrella',\n",
              " 'exhibition',\n",
              " 'dog',\n",
              " 'leash',\n",
              " 'safe',\n",
              " 'ceiling',\n",
              " 'fan',\n",
              " 'located',\n",
              " 'island',\n",
              " 'pantyhose',\n",
              " 'year',\n",
              " 'we',\n",
              " 'call',\n",
              " 'pic',\n",
              " 'takes',\n",
              " 'majority',\n",
              " 'buttons',\n",
              " 'remote',\n",
              " 'controls',\n",
              " 'functions',\n",
              " 'show',\n",
              " 'tv',\n",
              " 'control',\n",
              " 'function',\n",
              " 'remotes',\n",
              " 'little',\n",
              " 'booze',\n",
              " 'skier',\n",
              " 'those',\n",
              " 'perfectly',\n",
              " 'lines',\n",
              " 'elephant',\n",
              " 'eating',\n",
              " 'near',\n",
              " 'african',\n",
              " 'indian',\n",
              " 'tusks',\n",
              " 'zoo',\n",
              " 'fake',\n",
              " 'strip',\n",
              " 'across',\n",
              " 'horse',\n",
              " 'different',\n",
              " 'field',\n",
              " 'blanket',\n",
              " 'poles',\n",
              " 'distance',\n",
              " 'carrot',\n",
              " 'multi-colored',\n",
              " 'its',\n",
              " 'pupils',\n",
              " 'bedspread',\n",
              " 'vegetable',\n",
              " 'tips',\n",
              " 'paws',\n",
              " 'solid',\n",
              " 'breed',\n",
              " 'fur',\n",
              " 'smelling',\n",
              " 'whiskers',\n",
              " 'nose',\n",
              " 'species',\n",
              " 'grass',\n",
              " 'quilt',\n",
              " 'daytime',\n",
              " 'eyes',\n",
              " 'cats',\n",
              " 'creature',\n",
              " 'stuck',\n",
              " 'pieces',\n",
              " 'cloth',\n",
              " 'cow',\n",
              " 'falling',\n",
              " 'asleep',\n",
              " 'chasing',\n",
              " 'mouse',\n",
              " 'likely',\n",
              " 'basement',\n",
              " 'beds',\n",
              " 'rug',\n",
              " 'pillows',\n",
              " 'match',\n",
              " 'bed',\n",
              " 'sheet',\n",
              " 'dark',\n",
              " 'aircraft',\n",
              " 'lens',\n",
              " 'airport',\n",
              " 'numbers',\n",
              " 'predominant',\n",
              " 'shot',\n",
              " 'utensils',\n",
              " 'placemat',\n",
              " 'plate',\n",
              " 'sofa',\n",
              " 'baby',\n",
              " 'electronic',\n",
              " 'present',\n",
              " 'tvs',\n",
              " 'around',\n",
              " 'christmas',\n",
              " 'decoration',\n",
              " 'corner',\n",
              " 'shade',\n",
              " 'laying',\n",
              " 'bedroom',\n",
              " 'neat',\n",
              " 'pictures',\n",
              " 'lamp',\n",
              " '7',\n",
              " 'organization',\n",
              " 'logo',\n",
              " 'banner',\n",
              " 'raw',\n",
              " 'knife',\n",
              " 'main',\n",
              " 'crop',\n",
              " 'meat',\n",
              " 'through',\n",
              " 'door',\n",
              " 'backsplash',\n",
              " 'against',\n",
              " 'piano',\n",
              " 'inner',\n",
              " 'closed',\n",
              " 'traveling',\n",
              " 'location',\n",
              " 'much',\n",
              " 'luggage',\n",
              " 'foods',\n",
              " 'into',\n",
              " 'wine',\n",
              " 'considered',\n",
              " 'infant',\n",
              " 'lunch',\n",
              " 'reflected',\n",
              " 'couch',\n",
              " 'stands',\n",
              " 'job',\n",
              " 'clean',\n",
              " 'shaven',\n",
              " 'gentlemen',\n",
              " 'opposite',\n",
              " 'road',\n",
              " 'racers',\n",
              " 'their',\n",
              " 'heads',\n",
              " 'sporting',\n",
              " 'event',\n",
              " 'track',\n",
              " 'called',\n",
              " 'participating',\n",
              " 'covering',\n",
              " 'emotion',\n",
              " 'expressing',\n",
              " 'rocks',\n",
              " 'as',\n",
              " 'adult',\n",
              " 'trick',\n",
              " 'drinking',\n",
              " 'walls',\n",
              " 'nighttime',\n",
              " 'beverages',\n",
              " 'old-fashioned',\n",
              " 'paneling',\n",
              " 'owned',\n",
              " 'couple',\n",
              " 'presented',\n",
              " 'married',\n",
              " 'ride',\n",
              " 'handbag',\n",
              " 'using',\n",
              " 'seek',\n",
              " 'shelter',\n",
              " 'broken',\n",
              " 'outside',\n",
              " 'largest',\n",
              " 'part',\n",
              " 'kite',\n",
              " 'hydrant',\n",
              " 'formation',\n",
              " 'fighter',\n",
              " 'jets',\n",
              " 'mother',\n",
              " 'son',\n",
              " 'elephants',\n",
              " 'walking',\n",
              " 'hot',\n",
              " 'dogs',\n",
              " 'eyeglasses',\n",
              " 'dentist',\n",
              " 'recommend',\n",
              " 'use',\n",
              " 'product',\n",
              " 'foreground',\n",
              " 'handle',\n",
              " 'trees',\n",
              " 'bare',\n",
              " 'tracks',\n",
              " 'skiing',\n",
              " 'athletic',\n",
              " 'zebras',\n",
              " 'zebra',\n",
              " 'mowed',\n",
              " 'recently',\n",
              " 'colored',\n",
              " 'white',\n",
              " 'plants',\n",
              " 'count',\n",
              " 'already',\n",
              " 'earring',\n",
              " 'biting',\n",
              " 'chew',\n",
              " 'materials',\n",
              " 'makes',\n",
              " 'photobomb',\n",
              " 'item',\n",
              " 'consuming',\n",
              " 'tongs',\n",
              " 'baskets',\n",
              " 'bagels',\n",
              " 'fancy',\n",
              " 'dinner',\n",
              " 'home',\n",
              " 'object',\n",
              " 'shoes',\n",
              " 'upside',\n",
              " 'mugs',\n",
              " 'toaster',\n",
              " 'counter',\n",
              " 'wind',\n",
              " 'too',\n",
              " 'flippers',\n",
              " 'bench',\n",
              " 'seated',\n",
              " 'benches',\n",
              " 'leafs',\n",
              " 'thing',\n",
              " 'beach',\n",
              " 'engaging',\n",
              " 'three',\n",
              " 'bricks',\n",
              " 'swift',\n",
              " 'current',\n",
              " 'soda',\n",
              " 'served',\n",
              " 'heat',\n",
              " 'microwave',\n",
              " 'need',\n",
              " 'chopstick',\n",
              " 'desserts',\n",
              " 'items',\n",
              " 'touching',\n",
              " 'basketball',\n",
              " 'soccer',\n",
              " 'dribbling',\n",
              " 'material',\n",
              " 'goes',\n",
              " 'bins',\n",
              " 'skateboards',\n",
              " 'mouth',\n",
              " 'glasses',\n",
              " 'brown',\n",
              " 'short',\n",
              " 'finger',\n",
              " 'pointing',\n",
              " 'rain',\n",
              " 'numbered',\n",
              " 'waving',\n",
              " 'goodbye',\n",
              " 'somebody',\n",
              " 'flooded',\n",
              " 'busy',\n",
              " 'barefoot',\n",
              " 'protected',\n",
              " 'steam',\n",
              " 'engine',\n",
              " 'wild',\n",
              " 'life',\n",
              " 'jackets',\n",
              " 'boaters',\n",
              " 'fish',\n",
              " 'cloudy',\n",
              " 'boats',\n",
              " 'had',\n",
              " 'children',\n",
              " 'fight',\n",
              " 'gets',\n",
              " 'towel',\n",
              " 'rim',\n",
              " 'fluffy',\n",
              " 'bears',\n",
              " 'ripe',\n",
              " 'banana',\n",
              " 'fruits',\n",
              " 'florida',\n",
              " 'sold',\n",
              " 'characters',\n",
              " 'costume',\n",
              " 'interests',\n",
              " 'nametag',\n",
              " 'display',\n",
              " 'plugged',\n",
              " 'stickers',\n",
              " 'computer',\n",
              " 'girls',\n",
              " 'politician',\n",
              " 'favor',\n",
              " 'demonstrate',\n",
              " 'owner',\n",
              " 'suitcase',\n",
              " 'traveled',\n",
              " 'underground',\n",
              " 'station',\n",
              " 'getting',\n",
              " 'slower',\n",
              " 'construction',\n",
              " 'moved',\n",
              " 'another',\n",
              " 'site',\n",
              " 'easily',\n",
              " 'prepare',\n",
              " 'rings',\n",
              " 'jewelry',\n",
              " 'earrings',\n",
              " 'doughnut',\n",
              " 'only',\n",
              " 'pregnant',\n",
              " 'running',\n",
              " 'continent',\n",
              " 'found',\n",
              " 'snowboarding',\n",
              " 'brightest',\n",
              " 'jacket',\n",
              " 'snowboards',\n",
              " 'excited',\n",
              " 'kid',\n",
              " 'sleeping',\n",
              " 'awake',\n",
              " 'sheets',\n",
              " 'under',\n",
              " '1',\n",
              " 'talking',\n",
              " 'kids',\n",
              " 'related',\n",
              " 'babies',\n",
              " 'blinds',\n",
              " 'sharing',\n",
              " 'pants',\n",
              " '2',\n",
              " 'emotions',\n",
              " 'desk',\n",
              " 'laptops',\n",
              " 'grapes',\n",
              " 'bananas',\n",
              " 'dolphins',\n",
              " 'alone',\n",
              " 'comfortable',\n",
              " 'kites',\n",
              " 'clear',\n",
              " 'each',\n",
              " 'closest',\n",
              " 'indoor',\n",
              " 'skate',\n",
              " 'park',\n",
              " 'stepping',\n",
              " 'pink',\n",
              " 'training',\n",
              " 'port',\n",
              " 'potty',\n",
              " 'tiny',\n",
              " 'portion',\n",
              " 'miniature',\n",
              " 'floor',\n",
              " 'good',\n",
              " 'health',\n",
              " 'smiling',\n",
              " 'toy',\n",
              " 'feel',\n",
              " 'smooth',\n",
              " 'deal',\n",
              " 'offered',\n",
              " 'shop',\n",
              " 'i',\n",
              " 'buy',\n",
              " 'bathrobe',\n",
              " 'decorations',\n",
              " 'missing',\n",
              " 'teddy',\n",
              " 'normally',\n",
              " 'jumpers',\n",
              " 'alive',\n",
              " 'nosey',\n",
              " 'putting',\n",
              " 'furniture',\n",
              " 'candles',\n",
              " 'curly',\n",
              " 'four',\n",
              " 'cake',\n",
              " 'intact',\n",
              " 'containers',\n",
              " 'big',\n",
              " 'resting',\n",
              " 'company',\n",
              " 'represent',\n",
              " 'transporting',\n",
              " 'goods',\n",
              " 'branch',\n",
              " 'airplane',\n",
              " 'flipping',\n",
              " 'hoodie',\n",
              " 'far',\n",
              " 'prey',\n",
              " 'mate',\n",
              " 'suit',\n",
              " 'dirty',\n",
              " 'towed',\n",
              " 'surfers',\n",
              " 'juvenile',\n",
              " 'smallest',\n",
              " 'vending',\n",
              " 'machine',\n",
              " 'apparel',\n",
              " 'not',\n",
              " 'based',\n",
              " 'star',\n",
              " 'sidewalk',\n",
              " 'california',\n",
              " 'rainy',\n",
              " 'angry',\n",
              " 'welcomes',\n",
              " 'mild',\n",
              " 'winter',\n",
              " 'waiting',\n",
              " 'phones',\n",
              " 'clothes-shop',\n",
              " 'yellow',\n",
              " 'chest',\n",
              " 'way',\n",
              " 'allowed',\n",
              " 'radio',\n",
              " 'driver',\n",
              " 'countryside',\n",
              " 'separates',\n",
              " 'greenery',\n",
              " 'country',\n",
              " 'double',\n",
              " 'decker',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "metadata": {
        "id": "VlFUJttWD12m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ]
    },
    {
      "metadata": {
        "id": "-GIOx0u4DfzL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ]
    }
  ]
}